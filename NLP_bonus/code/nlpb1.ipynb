{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I want to do is to analyze the dataset I'm facing to understand which model to choose and how parametrize its input. To avoid to spend much time on this step I'll use \"pandas\" to import and collect statistics. I want to remove also as soon as possible all the stopwords using \"nltk\" to have a clearer idea on how many words in each text are really important. [I used the code on the NLP slide for stopwords, if you have an error as I had probably you need to download different resources with: (as suggested by the python error message)\n",
    "- nltk.download('stopwords')\n",
    "- nltk.download('punkt')]\n",
    "\n",
    "I'll use for word embedding \"gensim\", during the analysis of the text I'll take care only of words that appears in there if it contains almost all the words in our training texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import contractions\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fda66d2e3b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(21368330231508068)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this contains all the embedding for our words\n",
    "embed=api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as I said to run if you don't have stopwords already downloaded.\n",
    "# nltk.download('stopwords') \n",
    "# nltk.download('punkt')\n",
    "def tokenize_and_remove(data_path,embed):\n",
    "    '''to load data in \"data_path\", tokenize only words in \"embed\" and make small analysis adding columns of length for texts and tokens'''\n",
    "    sw = stopwords.words(\"english\")\n",
    "    df = pd.read_json(data_path,lines=True)\n",
    "    #this list will become a new column of our dataframe after fixing contractions and tokenize each text removing stopwords\n",
    "    token_cols=[]\n",
    "    #to do statistics on them\n",
    "    text_len=[]\n",
    "    token_len=[]\n",
    "    for idx in tqdm(range(len(df['text']))):\n",
    "        #tokenize and remove w classified as english stopwords or not in gensim dictionary we adopted\n",
    "        line=df['text'][idx]\n",
    "        text_len.append(len(line))\n",
    "        line=contractions.fix(line)\n",
    "        line=[w for w in word_tokenize(line) if not w.lower() in sw and w.isalpha() and w in embed]\n",
    "        token_len.append(len(line))\n",
    "        token_cols.append(line)\n",
    "    df['tokens']=token_cols\n",
    "    df['tokens_len']=token_len\n",
    "    df['text_len']=text_len\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186282/186282 [01:00<00:00, 3073.30it/s]\n"
     ]
    }
   ],
   "source": [
    "data='../data/train.jsonl'\n",
    "df=tokenize_and_remove(data,embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            text_len     tokens_len\n",
      "count  186282.000000  186282.000000\n",
      "mean      287.790823      27.730130\n",
      "std       100.141311       8.966043\n",
      "min        22.000000       2.000000\n",
      "25%       216.000000      22.000000\n",
      "50%       268.000000      27.000000\n",
      "75%       352.000000      33.000000\n",
      "max      8267.000000     870.000000\n"
     ]
    }
   ],
   "source": [
    "print(df[['text_len','tokens_len']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas \"describe()\" I'm trying to understand better this huge dataset according to the datas we had.\n",
    "I repeated the expirement also without filtering out the ones not in embed, in that case the statistics were:\n",
    "- count  186282.000000  186282.000000\n",
    "- mean      287.790823      29.307706\n",
    "- std       100.141311       9.675957\n",
    "- min        22.000000       3.000000\n",
    "- 25%       216.000000      23.000000\n",
    "- 50%       268.000000      28.000000\n",
    "- 75%       352.000000      35.000000\n",
    "- max      8267.000000     917.000000\n",
    "\n",
    "since they are not really different (27 vs 29 avg) I decided to keep this instance of gensim to handle my word embeddings.\n",
    "We can see that after the tokenization and removing of stop and not-embedded words the average of words in each text is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For curiosity I want to check the most frequent words in all out documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "words_count={}\n",
    "for line in df['tokens']:\n",
    "    for w in line:\n",
    "        words_count[w] = words_count.get(w, 0)+1\n",
    "words_count = dict(sorted(words_count.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 121011 different tokens.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(words_count.keys()), \"different tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said 27597\n",
      "new 18238\n",
      "one 17369\n",
      "Reuters 14430\n",
      "AP 14208\n",
      "first 13895\n",
      "people 12915\n",
      "New 12831\n",
      "would 12716\n",
      "two 12221\n"
     ]
    }
   ],
   "source": [
    "n=10 #n most common words\n",
    "for key in words_count.keys():\n",
    "    print(key,words_count[key])\n",
    "    n-=1\n",
    "    if not n:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I want to exploit the work done so far to encode the text in a 300 dim vector summing the results of all the value of the words not removed. I tought also about on concatenate the say n (maybe 20 considring that 75%+ of the texts have at least 20 words) most common words adding 0-padding where there was not possible to have n words keeping the sum of all the others, but maybe I'll dedicate to this solution later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilities to retrive and encode labels\n",
    "def label_to_idx(label):\n",
    "    labels_dict={\"business\":0, \"crime\":1, \"culture/arts\":2, \"education\":3, \"entertainment\":4,\n",
    "                \"environment\":5, \"food/drink\":6, \"home/living\":7, \"media\":8, \"politics\":9, \n",
    "                \"religion\":10, \"sci/tech\":11, \"sports\":12, \"wellness\":13, \"world\":14}\n",
    "    return labels_dict[label]\n",
    "def idx_to_label(idx):\n",
    "    labels_list=[\"business\", \"crime\", \"culture/arts\", \"education\", \"entertainment\",\n",
    "                \"environment\", \"food/drink\", \"home/living\", \"media\", \"politics\", \n",
    "                \"religion\", \"sci/tech\", \"sports\", \"wellness\", \"world\"]\n",
    "    return labels_list[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows to encode the vectors (by summing the contributes of each word token) given the dataframe \"df\" as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def encode_text(self,df,embed): #it also encode labels if labest parameter is set to true\n",
    "        data=[]\n",
    "        for idx,line in df.iterrows():\n",
    "            t=torch.zeros(300,)\n",
    "            for w in line['tokens']:\n",
    "                t+=torch.from_numpy(embed[w].copy())\n",
    "            l=torch.tensor(label_to_idx(line['label']),dtype=torch.int64) #only the label since we are going to use crossentropy\n",
    "            # print(\"label\",l)\n",
    "            data.append({'id': line['id'],'inputs':t,'outputs':l})\n",
    "        return data     \n",
    "    #without output lables_forward_reduce_cuda_kernel_1d_index\n",
    "    def encode_text_simple(self,df,embed):\n",
    "        data=[]\n",
    "        for idx,line in df.iterrows():\n",
    "            t=torch.zeros(300,)\n",
    "            for w in line['tokens']:\n",
    "                t+=torch.from_numpy(embed[w].copy())\n",
    "            data.append({'id': line['id'],'inputs':t})\n",
    "        return data     \n",
    "        \n",
    "    def __init__(self, df, embed, labels = True):\n",
    "        self.labels=labels\n",
    "        if labels:\n",
    "            self.data=self.encode_text(df,embed)\n",
    "        else:\n",
    "            self.data=self.encode_text_simple(df,embed)\n",
    "        self.num_samples = len(self.data)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is ready rather than going for ML models like svm that Iìve already used in the machine learning course I decided to practice in neural networks developing an artificial neural network (multi layer perceptron). I decided also to use the trainer class that the professor show us almost untouched for what concern the training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module): #an ANN\n",
    "    def __init__(self,input_dim,output_dim,hparams=None):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): is the input dimension of our network\n",
    "            output_dim (int): output dimension of our networks, the number of classes.\n",
    "            hparams (optional): the hyperparameters of our model. Defaults to None for now,\n",
    "                    maybe in future extensions I could add here the parameter for which I'm dividing (2)\n",
    "        \"\"\"\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.in_dim=input_dim\n",
    "        self.out_dim=output_dim\n",
    "        self.body=nn.ModuleList()\n",
    "        i=input_dim\n",
    "        while(i>4*output_dim):\n",
    "            self.body.append(nn.Sequential(\n",
    "                nn.Linear(i,int(i/2)),\n",
    "                nn.ReLU(),  \n",
    "                nn.Dropout(0.2) \n",
    "            ))\n",
    "            i=int(i/2)\n",
    "        self.final=nn.Sequential(\n",
    "            nn.Linear(i,int(i/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(i/2),output_dim),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.body):\n",
    "            x=l(x)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"Utility class to train and evaluate a model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        loss_function,\n",
    "        optimizer,\n",
    "        device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: the model we want to train.\n",
    "            loss_function: the loss_function to minimize.\n",
    "            optimizer: the optimizer used to minimize the loss_function.\n",
    "            device: cuda or cpu depending on where our training will be performed.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.device=device\n",
    "        self.model.to(self.device)  # move model to GPU if available\n",
    "\n",
    "    def train(self, train_dataset, valid_dataset, epochs=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_dataset: a Dataset or DatasetLoader instance containing the training instances.\n",
    "            valid_dataset: a Dataset or DatasetLoader instance used to evaluate learning progress.\n",
    "            epochs: the number of times to iterate over train_dataset.\n",
    "        Returns:\n",
    "            avg_train_loss: the average training loss on train_dataset over\n",
    "                epochs.\n",
    "        \"\"\"\n",
    "        assert epochs > 1 and isinstance(epochs, int)\n",
    "        print('Training...')\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            print(' Epoch {:03d}'.format(epoch + 1))\n",
    "            epoch_loss = 0.0\n",
    "            for _, sample in enumerate(train_dataset):\n",
    "                inputs = sample['inputs'].to(self.device)\n",
    "                labels = sample['outputs'].to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                predictions = self.model(inputs)               \n",
    "                sample_loss = self.loss_function(predictions, labels)\n",
    "                sample_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # sample_loss is a Tensor\n",
    "                epoch_loss += sample_loss.tolist()\n",
    "            \n",
    "            avg_epoch_loss = epoch_loss / len(train_dataset)\n",
    "            train_loss += avg_epoch_loss\n",
    "            print('  [E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
    "\n",
    "            valid_loss = self.evaluate(valid_dataset)\n",
    "            \n",
    "            print('  [E: {:2d}] valid loss = {:0.4f}'.format(epoch, valid_loss))\n",
    "            ## these two lines can be removed, I used them to check how good we are doing at each epochs\n",
    "            save_results(data_dev,\"../predictions/nnbv1_dev.tsv\",self)\n",
    "            !python3 ../scorer.py --prediction_file ../predictions/nnbv1_dev.tsv --gold_file ../gold/gold_dev.tsv\n",
    "        print('... Done!')\n",
    "    \n",
    "        avg_epoch_loss = train_loss / epochs\n",
    "        return avg_epoch_loss\n",
    "    \n",
    "\n",
    "    def evaluate(self, valid_dataset):\n",
    "        # self.model.eval()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            valid_dataset: the dataset to use to evaluate the model.\n",
    "\n",
    "        Returns:\n",
    "            avg_valid_loss: the average validation loss over valid_dataset.\n",
    "        \"\"\"\n",
    "        valid_loss = 0.0\n",
    "        # no gradient updates here\n",
    "        with torch.no_grad():\n",
    "            for sample in valid_dataset:\n",
    "                inputs = sample['inputs'].to(self.device)\n",
    "                labels = sample['outputs'].to(self.device)\n",
    "                predictions = self.model(inputs)\n",
    "                # print(\"pred\",predictions,\"labels\",labels)\n",
    "                sample_loss = self.loss_function(predictions, labels)\n",
    "                valid_loss += sample_loss.tolist()\n",
    "        \n",
    "        return valid_loss / len(valid_dataset)\n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        # self.model.eval()\n",
    "        \"\"\"\n",
    "        Returns: hopefully the right prediction.\n",
    "        \"\"\"\n",
    "        res=[]\n",
    "        output=self.model(x.to(self.device))\n",
    "        for y in output:\n",
    "            max=idx_to_label(torch.argmax(y))\n",
    "            res.append(max)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the Datasets and create the trainer with model, loss and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 279/6844 [00:00<00:02, 2786.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186282\n",
      "{'id': 34826, 'inputs': tensor([ 0.5984,  2.1544, -2.1497,  3.4221, -1.0396, -1.1547,  1.5708, -4.1302,\n",
      "         0.7389,  3.0746, -0.8054, -3.1855,  1.3969,  2.0278, -2.7422,  2.1461,\n",
      "         3.3774,  5.3034,  1.4444, -1.1899, -0.1482,  2.1570,  0.6310, -0.3157,\n",
      "         0.8607, -1.3098, -1.0935,  2.7092,  2.1862, -0.5253, -0.6049,  0.3447,\n",
      "         0.5847,  2.0790,  0.7202, -0.6893,  2.6213, -1.6462, -0.3467,  2.2975,\n",
      "         3.0027, -2.3688,  2.2446, -2.7668,  1.4780, -1.9380, -1.8821,  0.6446,\n",
      "         3.0092,  2.9491, -1.9568,  2.6705, -1.5483, -1.2118,  0.0370, -1.3667,\n",
      "        -1.7024, -3.1867, -1.5602, -3.4138,  0.6928,  2.0115, -2.8325, -2.8777,\n",
      "        -0.6772,  1.8848, -2.3920,  2.5192,  0.2378,  2.8743,  2.8725,  1.1666,\n",
      "         1.1083,  0.7915, -4.2026,  0.6115,  1.8562,  3.4143,  2.0905,  4.6426,\n",
      "         0.8409, -2.3152, -0.5070, -1.3265, -0.8783, -0.2410, -1.8527,  3.2277,\n",
      "         1.3466,  1.1963,  0.2794,  0.3676, -1.9456, -2.5863, -1.2511, -1.0417,\n",
      "         3.3648,  1.1414, -0.3565, -3.9185, -1.3052, -1.7812,  0.4233,  0.0737,\n",
      "        -1.3205, -1.2498,  1.9641,  0.2075,  0.3869, -0.0298, -1.5511,  0.4594,\n",
      "         0.3236,  0.0295,  2.0812, -1.1315,  0.5290, -1.3267,  2.3788,  1.9381,\n",
      "        -4.4501, -0.1550, -0.0643,  1.2896,  0.4940,  0.8950, -2.3958, -0.3775,\n",
      "         1.7010,  1.4457, -1.7404, -4.2146, -3.7179,  1.4811, -0.9822,  0.5177,\n",
      "         0.7072,  0.4326, -0.7347,  1.4850,  1.5112, -1.3870, -0.5471, -0.1811,\n",
      "         1.0245,  2.9703, -0.4332,  0.1095, -1.3228, -1.1246,  1.9870,  1.1012,\n",
      "        -1.2386,  3.4896, -1.5258, -1.1689, -2.2048, -2.7602, -1.4308, -1.1708,\n",
      "         0.9089,  2.4347,  2.9009,  1.7432,  0.6940, -3.5045,  1.8650, -2.0951,\n",
      "        -1.1163,  1.0352, -2.0442, -1.3396,  1.4323, -2.0173,  0.5032,  1.0758,\n",
      "         3.8219, -1.9454, -0.6665,  0.7179, -2.6724, -1.3907,  1.2007,  0.6687,\n",
      "        -2.1334, -1.8116,  0.2182, -0.9075,  1.1251,  0.8962,  0.9702,  1.0559,\n",
      "         1.9270, -0.6933, -1.6062,  2.0375,  1.0710,  0.6398, -2.0970, -1.3229,\n",
      "         0.2940,  0.8763, -2.2772,  1.2379,  2.5542, -0.0957, -2.6974,  0.6293,\n",
      "        -1.7135, -0.2105, -2.2911,  0.2756, -1.1304,  0.8480, -3.1335, -2.1230,\n",
      "         0.7603,  0.7502, -3.9511, -0.8907, -1.1474,  1.9815,  1.7009, -0.1078,\n",
      "         2.1233, -0.8276,  0.5407, -1.9374, -0.7248, -1.3813, -0.6961,  0.3505,\n",
      "         0.1765,  2.7395,  2.3328,  1.4002, -0.1488, -3.6369,  1.4546,  0.0812,\n",
      "         2.8249, -0.5590, -1.2463, -2.3902, -1.7467,  2.3035,  0.4020,  2.5808,\n",
      "        -0.6742, -0.6273,  0.7947,  2.9214,  2.6349,  4.9740,  0.5135, -1.1159,\n",
      "         1.4537,  0.3799, -2.5035, -0.9641, -1.4187, -0.4569, -2.2809, -0.7829,\n",
      "         0.8606,  4.5651, -1.2387, -0.6627, -1.3738,  1.5987, -1.2238,  3.6473,\n",
      "         1.3679,  1.9167,  2.8137, -1.0444, -1.2778, -1.9407, -0.2795,  0.0525,\n",
      "         1.0510, -0.9963,  1.1640,  1.0104, -0.0326,  0.4312, -3.2879,  0.8853,\n",
      "         0.7046,  2.2620, -1.3384,  1.0420, -3.4915, -1.1885, -2.1689, -1.7302,\n",
      "         2.6487, -0.5439,  1.0892,  1.3902]), 'outputs': tensor(2)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6844/6844 [00:02<00:00, 2749.59it/s]\n",
      "100%|██████████| 6849/6849 [00:02<00:00, 3241.31it/s]\n"
     ]
    }
   ],
   "source": [
    "dftrain=TextDataset(df,embed,True)\n",
    "print(len(dftrain))\n",
    "print(dftrain[3])\n",
    "# load and encode dev and test dataset too\n",
    "dev='../data/dev.jsonl'\n",
    "dfde=tokenize_and_remove(dev,embed)\n",
    "dfdev=TextDataset(dfde,embed,True)\n",
    "\n",
    "test='../data/test.jsonl'\n",
    "dfte=tokenize_and_remove(test,embed)\n",
    "dftest=TextDataset(dfte,embed,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=DataLoader(dftrain,batch_size=128,num_workers=6)\n",
    "data_dev=DataLoader(dfdev,batch_size=10)\n",
    "data_test=DataLoader(dftest,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "mod=TextClassifier(300,15)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mod.parameters(), lr=1e-5)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "trainer=Trainer(mod,loss,optimizer,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimizer I run tons of different combinations, not only for what concern the learning rate but also with weight decay and momentums, the following pth is the best I was able to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run only if you want to start with a pre-trained model resulting from another epoch\n",
    "trainer.model.load_state_dict(torch.load(\"nnbonusv1_400+100A.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility to save results\n",
    "def save_results(dataset,path,trainer):\n",
    "    output = open(path,\"w\")\n",
    "    for x in dataset:\n",
    "        y=trainer.predict(x['inputs'])\n",
    "        for a,id in zip(y,x['id']):\n",
    "            print(str(id.item())+\"\\t\"+a,file=output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model I was able to obtain is the one that results from training for 400 epochs uning SGD and then switching to Adam optimizer for 100 more.\n",
    "\n",
    "(I used a really small lr=1e-5, probably with an higher one way less epochs would have been needed)\n",
    "\n",
    "Weights of that model can be load above \"nnbonusv1_400+100A.pth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(data_train,data_dev,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.loss = optim.Adam(trainer.model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(data_train,data_dev,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.mod.load_state_dict(torch.load(\"nnbonusv1_400+100A.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701703387564116"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.evaluate(data_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results of our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'err_rate': '21.41'}\n"
     ]
    }
   ],
   "source": [
    "save_results(data_dev,\"../predictions/nnbv1_dev.tsv\",trainer)\n",
    "!python3 ../scorer.py --prediction_file ../predictions/nnbv1_dev.tsv --gold_file ../gold/gold_dev.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(data_test,\"../predictions/predictions_test.tsv\",trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mod.state_dict(), \"nnbonusv1_seed200.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.seed()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
